{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fa8493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1587577641674e52aa4144c2336c1d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='samochód syrena na ulicy', description='Prompt:', layout=Layout(width='900px')), Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42c5a3d73b8462eb502485db5cbcc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Wyszukiwanie obrazów po promptcie CLIP (text->image) na bazie gotowych embeddingów.\n",
    "\n",
    "    Wymaga:\n",
    "    - CSV z kolumnami:\n",
    "        \"Cutout Image Path\"\n",
    "        \"openai/clip-vit-base-patch32_Embedding\" (JSON string list[float], znormalizowane lub nie)\n",
    "    - transformers (CLIPModel, CLIPProcessor)\n",
    "\n",
    "    UI:\n",
    "    - pole tekstowe (prompt)\n",
    "    - przycisk \"Szukaj\"\n",
    "    - wynik: TOP 20 miniatur + similarity\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import io\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# --- ścieżki / kolumny ---\n",
    "csv_path = Path(\"/Users/olga/MetaLogic/outputs/cutouts_with_embeddings.csv\")\n",
    "IMG_COL = \"Cutout Image Path\"\n",
    "EMB_COL = \"openai/clip-vit-base-patch32_Embedding\"\n",
    "\n",
    "# --- model ---\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- wczytanie embeddingów obrazów (raz) ---\n",
    "df = pd.read_csv(csv_path)\n",
    "emb_list = df[EMB_COL].apply(json.loads).tolist()\n",
    "E = np.vstack(emb_list).astype(np.float32)  # (N, 512)\n",
    "\n",
    "# dla bezpieczeństwa: normalizacja (jeśli już były znormalizowane, nic nie psuje)\n",
    "E /= (np.linalg.norm(E, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def _thumb_bytes(p: Path, size=(520, 520)):\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    img.thumbnail(size)\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"JPEG\", quality=85)\n",
    "    return buf.getvalue()\n",
    "\n",
    "# --- UI ---\n",
    "txt = W.Text(value=\"samochód syrena na ulicy\", description=\"Prompt:\", layout=W.Layout(width=\"900px\"))\n",
    "btn = W.Button(description=\"Szukaj (CLIP)\", button_style=\"primary\")\n",
    "out = W.Output()\n",
    "\n",
    "def _search(_):\n",
    "    prompt = (txt.value or \"\").strip()\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        if not prompt:\n",
    "            print(\"Wpisz prompt.\")\n",
    "            return\n",
    "\n",
    "        # embedding tekstu\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(text=[prompt], return_tensors=\"pt\", padding=True).to(device)\n",
    "            t = model.get_text_features(**inputs)\n",
    "            t = t / t.norm(dim=-1, keepdim=True)\n",
    "            t = t.detach().cpu().float().numpy().reshape(-1)  # (512,)\n",
    "\n",
    "        # cosine similarity = dot product (po normalizacji)\n",
    "        sims = E @ t  # (N,)\n",
    "        top_idx = np.argsort(-sims)[:20]\n",
    "\n",
    "        df_top = df.iloc[top_idx].copy()\n",
    "        df_top[\"Similarity\"] = sims[top_idx]\n",
    "\n",
    "        # wizualizacja: 2 kolumny, 10 wierszy\n",
    "        cards = []\n",
    "        for rank, (irow, row) in enumerate(df_top.iterrows(), start=1):\n",
    "            p = Path(str(row[IMG_COL]))\n",
    "            sim = float(row[\"Similarity\"])\n",
    "            try:\n",
    "                im = W.Image(value=_thumb_bytes(p), format=\"jpeg\", width=520, height=520)\n",
    "            except Exception:\n",
    "                im = W.Label(f\"Nie można otworzyć: {p}\")\n",
    "\n",
    "            title = W.HTML(f\"<b>{rank:02d}. sim={sim:.3f} — {p.name}</b><br><code>{p}</code>\")\n",
    "            cards.append(W.VBox([title, im], layout=W.Layout(border=\"1px solid #ddd\", padding=\"8px\")))\n",
    "\n",
    "        rows = []\n",
    "        for r in range(0, len(cards), 2):\n",
    "            rows.append(W.HBox(cards[r:r+2], layout=W.Layout(gap=\"12px\")))\n",
    "        display(W.VBox(rows, layout=W.Layout(gap=\"12px\")))\n",
    "\n",
    "btn.on_click(_search)\n",
    "display(W.HBox([txt, btn]), out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv MetaLogic)",
   "language": "python",
   "name": "metalogic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
